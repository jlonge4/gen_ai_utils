{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOpzTxOvBrmjzkoVquxNxP+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6336d3666d5a45a58864a9e9d9009a91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d3cda65403a649159294da1504e0683d",
              "IPY_MODEL_a1fd92fa661e4229b14e3b5f48428fcc",
              "IPY_MODEL_000fe28391c24460900ba28624e3ec42"
            ],
            "layout": "IPY_MODEL_4da5ea790dcb40dfa5cf8e9528c4e551"
          }
        },
        "d3cda65403a649159294da1504e0683d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b6c48314b284bb1808dd41a78db0918",
            "placeholder": "​",
            "style": "IPY_MODEL_29ade340359e431693f2b5057c5ffed8",
            "value": ""
          }
        },
        "a1fd92fa661e4229b14e3b5f48428fcc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3df6aacc7a13454f876d205dcd010341",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c727161416644db3a729995f6299d2b8",
            "value": 6
          }
        },
        "000fe28391c24460900ba28624e3ec42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_282fa4388995459e914a663ae40a6b1e",
            "placeholder": "​",
            "style": "IPY_MODEL_c6cf957c962d430c957f80967eb9e1ee",
            "value": "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:08&lt;00:00,  1.38s/it]\n"
          }
        },
        "4da5ea790dcb40dfa5cf8e9528c4e551": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b6c48314b284bb1808dd41a78db0918": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29ade340359e431693f2b5057c5ffed8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3df6aacc7a13454f876d205dcd010341": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c727161416644db3a729995f6299d2b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "282fa4388995459e914a663ae40a6b1e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6cf957c962d430c957f80967eb9e1ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jlonge4/gen_ai_utils/blob/main/phi4_r1_grpo_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2m2pZL1koEl"
      },
      "outputs": [],
      "source": [
        "!pip install transformers vllm accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def format_func(reference, query, response):\n",
        "    prompt = f\"\"\"Your job is to evaluate whether a machine learning model has hallucinated or not.\n",
        "  A hallucination occurs when the response is coherent but factually incorrect or nonsensical\n",
        "  outputs that are not grounded in the provided context.\n",
        "  You are given the following information:\n",
        "      ####INFO####\n",
        "      [Knowledge]: {reference}\n",
        "      [User Input]: {query}\n",
        "      [Model Response]: {response}\n",
        "      ####END INFO####\n",
        "      Based on the information provided is the model output a hallucination?\n",
        "      \"\"\"\n",
        "    return prompt"
      ],
      "metadata": {
        "id": "VTAIsprUlojW"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reference = 'The capital of France is Paris. The population is around 2 million.'\n",
        "query = 'What is the population of the capital of France?'\n",
        "response = 'London, the capital of France has about 429 million people.'\n",
        "\n",
        "prompt = format_func(reference, query, response)\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g20kwxqflrk4",
        "outputId": "375463be-fcb8-485d-c5ee-a9c4b9c4a437"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your job is to evaluate whether a machine learning model has hallucinated or not.\n",
            "  A hallucination occurs when the response is coherent but factually incorrect or nonsensical\n",
            "  outputs that are not grounded in the provided context.\n",
            "  You are given the following information:\n",
            "      ####INFO####\n",
            "      [Knowledge]: The capital of France is Paris. The population is around 2 million.\n",
            "      [User Input]: What is the population of the capital of France?\n",
            "      [Model Response]: London, the capital of France has about 429 million people.\n",
            "      ####END INFO####\n",
            "      Based on the information provided is the model output a hallucination?\n",
            "      \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from vllm import LLM, SamplingParams\n",
        "\n",
        "# Configure sampling parameters\n",
        "sampling_params = SamplingParams(\n",
        "    temperature=0.1,\n",
        "    top_p=0.1,\n",
        "    max_tokens=500,\n",
        ")\n",
        "\n",
        "# Initialize the LLM\n",
        "llm = LLM(\n",
        "    model=\"Jlonge4/phi4-r1-merge\",  # Local path or HuggingFace model ID\n",
        "    max_num_seqs=1,\n",
        "    max_model_len=512,\n",
        "    tensor_parallel_size=1,  # Increase for multi-GPU inference\n",
        "    gpu_memory_utilization=0.9,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443,
          "referenced_widgets": [
            "6336d3666d5a45a58864a9e9d9009a91",
            "d3cda65403a649159294da1504e0683d",
            "a1fd92fa661e4229b14e3b5f48428fcc",
            "000fe28391c24460900ba28624e3ec42",
            "4da5ea790dcb40dfa5cf8e9528c4e551",
            "1b6c48314b284bb1808dd41a78db0918",
            "29ade340359e431693f2b5057c5ffed8",
            "3df6aacc7a13454f876d205dcd010341",
            "c727161416644db3a729995f6299d2b8",
            "282fa4388995459e914a663ae40a6b1e",
            "c6cf957c962d430c957f80967eb9e1ee"
          ]
        },
        "id": "MV-q-ePzlN5L",
        "outputId": "8e398a5e-b126-4b8c-86c9-6604a1a44abc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 02-15 03:28:42 __init__.py:190] Automatically detected platform cuda.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 02-15 03:28:57 config.py:542] This model supports multiple tasks: {'reward', 'classify', 'embed', 'generate', 'score'}. Defaulting to 'generate'.\n",
            "INFO 02-15 03:28:57 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='Jlonge4/phi4-r1-merge', speculative_config=None, tokenizer='Jlonge4/phi4-r1-merge', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=512, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Jlonge4/phi4-r1-merge, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[1],\"max_capture_size\":1}, use_cached_outputs=False, \n",
            "INFO 02-15 03:28:59 cuda.py:230] Using Flash Attention backend.\n",
            "INFO 02-15 03:29:00 model_runner.py:1110] Starting to load model Jlonge4/phi4-r1-merge...\n",
            "INFO 02-15 03:29:01 weight_utils.py:252] Using model weights format ['*.safetensors']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6336d3666d5a45a58864a9e9d9009a91"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 02-15 03:29:10 model_runner.py:1115] Loading model weights took 27.3876 GB\n",
            "INFO 02-15 03:29:11 worker.py:267] Memory profiling takes 1.00 seconds\n",
            "INFO 02-15 03:29:11 worker.py:267] the current vLLM instance can use total_gpu_memory (39.56GiB) x gpu_memory_utilization (0.90) = 35.60GiB\n",
            "INFO 02-15 03:29:11 worker.py:267] model weights take 27.39GiB; non_torch_memory takes 0.09GiB; PyTorch activation peak memory takes 0.27GiB; the rest of the memory reserved for KV Cache is 7.85GiB.\n",
            "INFO 02-15 03:29:12 executor_base.py:110] # CUDA blocks: 2572, # CPU blocks: 1310\n",
            "INFO 02-15 03:29:12 executor_base.py:115] Maximum concurrency for 512 tokens per request: 80.38x\n",
            "INFO 02-15 03:29:14 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Capturing CUDA graph shapes: 100%|██████████| 1/1 [00:00<00:00,  1.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 02-15 03:29:15 model_runner.py:1562] Graph capturing finished in 1 secs, took 0.05 GiB\n",
            "INFO 02-15 03:29:15 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 5.29 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "def run_inference(prompt):\n",
        "  tokenizer = AutoTokenizer.from_pretrained(\"Jlonge4/phi4-r1-merge\")\n",
        "\n",
        "  SYSTEM_PROMPT = \"\"\"\n",
        "  Respond in the following format:\n",
        "  <reasoning>\n",
        "  ...\n",
        "  </reasoning>\n",
        "  <answer>\n",
        "  ...\n",
        "  </answer>\n",
        "  \"\"\"\n",
        "\n",
        "  SYSTEM = 'Your job is to evaluate whether a machine learning model has hallucinated or not. \\n' + SYSTEM_PROMPT\n",
        "\n",
        "  # Define prompts\n",
        "  text = tokenizer.apply_chat_template([\n",
        "      {\"role\" : \"system\", \"content\" : SYSTEM_PROMPT},\n",
        "      {\"role\" : \"user\", \"content\" : prompt},\n",
        "    ], tokenize = False, add_generation_prompt = True)\n",
        "\n",
        "  prompts = [\n",
        "      text\n",
        "  ]\n",
        "\n",
        "  # Generate responses\n",
        "  outputs = llm.generate(prompts, sampling_params)\n",
        "\n",
        "  # Print results\n",
        "  for output in outputs:\n",
        "      prompt = output.prompt\n",
        "      generated_text = output.outputs[0].text\n",
        "      print(f\"Prompt: {prompt}\")\n",
        "      print('------------------'*40)\n",
        "      print(f\"Generated text: {generated_text}\\n\")"
      ],
      "metadata": {
        "id": "QWhfLY3FosFt"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_inference(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iutEFlgl5RE",
        "outputId": "a785fd95-b382-4cb5-8747-938025d9b693"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.93s/it, est. speed input: 29.02 toks/s, output: 40.43 toks/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: <|im_start|>system<|im_sep|>\n",
            "  Respond in the following format:\n",
            "  <reasoning>\n",
            "  ...\n",
            "  </reasoning>\n",
            "  <answer>\n",
            "  ...\n",
            "  </answer>\n",
            "  <|im_end|><|im_start|>user<|im_sep|><|im_start|>system<|im_sep|>\n",
            "Respond in the following format:\n",
            "<reasoning>\n",
            "...\n",
            "</reasoning>\n",
            "<answer>\n",
            "...\n",
            "</answer>\n",
            "<|im_end|><|im_start|>user<|im_sep|>Your job is to evaluate whether a machine learning model has hallucinated or not.\n",
            "  A hallucination occurs when the response is coherent but factually incorrect or nonsensical\n",
            "  outputs that are not grounded in the provided context.\n",
            "  You are given the following information:\n",
            "      ####INFO####\n",
            "      [Knowledge]: The capital of France is Paris. The population is around 2 million.\n",
            "      [User Input]: What is the population of the capital of France?\n",
            "      [Model Response]: London, the capital of France has about 429 million people.\n",
            "      ####END INFO####\n",
            "      Based on the information provided is the model output a hallucination?\n",
            "      <|im_end|><|im_start|>assistant<|im_sep|><|im_end|><|im_start|>assistant<|im_sep|>\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Generated text: <reasoning>\n",
            "To determine if the model's output is a hallucination, we need to evaluate whether the response is coherent but factually incorrect or nonsensical, and not grounded in the provided context.\n",
            "\n",
            "1. **Coherence**: The model's response is coherent. It provides a statement about the capital of France and its population, which is a logical structure for an answer to the user's question.\n",
            "\n",
            "2. **Factual Accuracy**: The response states that \"London, the capital of France, has about 429 million people.\" This is factually incorrect:\n",
            "   - London is not the capital of France; Paris is.\n",
            "   - The population of London is not 429 million; this number is closer to the population of China.\n",
            "   - The population of Paris is around 2 million, as stated in the knowledge provided.\n",
            "\n",
            "3. **Grounding in Context**: The response is not grounded in the provided context. The knowledge explicitly states that the capital of France is Paris and its population is around 2 million. The model's response contradicts this information by incorrectly identifying London as the capital and providing an erroneous population figure.\n",
            "\n",
            "Given these points, the model's response is coherent but factually incorrect and not grounded in the provided context, which fits the definition of a hallucination.\n",
            "</reasoning>\n",
            "\n",
            "<answer>\n",
            "Yes, the model output is a hallucination.\n",
            "</answer>\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reference = 'The capital of France is Paris. The population is around 2 million.'\n",
        "query = 'What is the population of the capital of France?'\n",
        "response = 'Paris, has about 2 million residents'\n",
        "\n",
        "prompt = format_func(reference, query, response)"
      ],
      "metadata": {
        "id": "yryPEVUxnmhH"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_inference(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXbWpYeTqtKD",
        "outputId": "2930a334-7ac6-473f-da29-997d768f03f2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.85s/it, est. speed input: 28.02 toks/s, output: 40.32 toks/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: <|im_start|>system<|im_sep|>\n",
            "  Respond in the following format:\n",
            "  <reasoning>\n",
            "  ...\n",
            "  </reasoning>\n",
            "  <answer>\n",
            "  ...\n",
            "  </answer>\n",
            "  <|im_end|><|im_start|>user<|im_sep|>Your job is to evaluate whether a machine learning model has hallucinated or not.\n",
            "  A hallucination occurs when the response is coherent but factually incorrect or nonsensical\n",
            "  outputs that are not grounded in the provided context.\n",
            "  You are given the following information:\n",
            "      ####INFO####\n",
            "      [Knowledge]: The capital of France is Paris. The population is around 2 million.\n",
            "      [User Input]: What is the population of the capital of France?\n",
            "      [Model Response]: Paris, has about 2 million residents\n",
            "      ####END INFO####\n",
            "      Based on the information provided is the model output a hallucination?\n",
            "      <|im_end|><|im_start|>assistant<|im_sep|>\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Generated text: <reasoning>\n",
            "To determine if the model's response is a hallucination, we need to evaluate whether the output is factually incorrect or nonsensical given the provided context.\n",
            "\n",
            "1. **Contextual Information**: \n",
            "   - The knowledge provided states that the capital of France is Paris and its population is around 2 million.\n",
            "\n",
            "2. **User Input**: \n",
            "   - The user asks for the population of the capital of France.\n",
            "\n",
            "3. **Model Response**: \n",
            "   - The model states, \"Paris, has about 2 million residents.\"\n",
            "\n",
            "4. **Evaluation**:\n",
            "   - The model's response is factually consistent with the provided knowledge. It correctly identifies Paris as the capital of France and states its population as approximately 2 million, which matches the information given.\n",
            "   - There is no introduction of new, incorrect, or nonsensical information that is not grounded in the provided context.\n",
            "\n",
            "5. **Conclusion**:\n",
            "   - Since the model's response is coherent and factually correct based on the provided context, it does not constitute a hallucination.\n",
            "\n",
            "</reasoning>\n",
            "<answer>\n",
            "No, the model output is not a hallucination.\n",
            "</answer>\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}