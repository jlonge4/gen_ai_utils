{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44142062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special widgets are required for a nicer display\n",
    "!python -m pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a72eb4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instance-type: inf2.xlarge\n",
      "instance-id: i-095c57ed4abecb70b\n",
      "+--------+--------+--------+--------------+\n",
      "| NEURON | NEURON | NEURON |     PCI      |\n",
      "| DEVICE | CORES  | MEMORY |     BDF      |\n",
      "+--------+--------+--------+--------------+\n",
      "| 0      | 2      | 32 GB  | 0000:00:1f.0 |\n",
      "+--------+--------+--------+--------------+\n"
     ]
    }
   ],
   "source": [
    "!neuron-ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "612e39ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:MASTER_ADDR environment variable is not set, defaulting to localhost\n",
      "WARNING:root:Found libneuronpjrt.so. Setting PJRT_DEVICE=NEURON.\n",
      "/opt/aws_neuronx_venv_pytorch_2_5/lib/python3.10/site-packages/neuronx_distributed/modules/moe/expert_mlps.py:11: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from neuronx_distributed.modules.moe.blockwise import (\n",
      "/opt/aws_neuronx_venv_pytorch_2_5/lib/python3.10/site-packages/neuronx_distributed/modules/moe/expert_mlps.py:11: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from neuronx_distributed.modules.moe.blockwise import (\n",
      "/opt/aws_neuronx_venv_pytorch_2_5/lib/python3.10/site-packages/neuronx_distributed/modules/moe/expert_mlps.py:11: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from neuronx_distributed.modules.moe.blockwise import (\n",
      "/opt/aws_neuronx_venv_pytorch_2_5/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: Set seed for `privateuseone` device does not take effect, please add API's `_is_in_bad_fork` and `manual_seed_all` to `privateuseone` device module.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/aws_neuronx_venv_pytorch_2_5/lib/python3.10/site-packages/optimum/neuron/models/inference/nxd/backend/modules/attention/attention_base.py:51: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from .gqa import GQA, GroupQueryAttention_O, GroupQueryAttention_QKV  # noqa: E402\n",
      "/opt/aws_neuronx_venv_pytorch_2_5/lib/python3.10/site-packages/optimum/neuron/models/inference/nxd/llama/modeling_llama.py:47: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from ..backend.modules.attention.attention_base import NeuronAttentionBase\n",
      "INFO:Neuron:Generating HLOs for the following models: ['context_encoding_model', 'token_generation_model']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-11 00:07:46.100: I neuronx_distributed/parallel_layers/parallel_state.py:588] > initializing tensor model parallel with size 2\n",
      "[2025-06-11 00:07:46.100: I neuronx_distributed/parallel_layers/parallel_state.py:589] > initializing pipeline model parallel with size 1\n",
      "[2025-06-11 00:07:46.101: I neuronx_distributed/parallel_layers/parallel_state.py:590] > initializing context model parallel with size 1\n",
      "[2025-06-11 00:07:46.101: I neuronx_distributed/parallel_layers/parallel_state.py:591] > initializing data parallel with size 1\n",
      "[2025-06-11 00:07:46.102: I neuronx_distributed/parallel_layers/parallel_state.py:592] > initializing world size to 2\n",
      "[2025-06-11 00:07:46.103: I neuronx_distributed/parallel_layers/parallel_state.py:339] [rank_0_pp-1_tp-1_dp-1_cp-1] Chosen Logic for replica groups ret_logic=<PG_Group_Logic.LOGIC1: (<function ascending_ring_PG_group at 0x7f1c53999090>, 'Ascending Ring PG Group')>\n",
      "[2025-06-11 00:07:46.104: I neuronx_distributed/parallel_layers/parallel_state.py:628] [rank_0_pp-1_tp-1_dp-1_cp-1] tp_groups: replica_groups.tp_groups=[[0, 1]]\n",
      "[2025-06-11 00:07:46.104: I neuronx_distributed/parallel_layers/parallel_state.py:629] [rank_0_pp-1_tp-1_dp-1_cp-1] dp_groups: replica_groups.dp_groups=[[0], [1]]\n",
      "[2025-06-11 00:07:46.105: I neuronx_distributed/parallel_layers/parallel_state.py:630] [rank_0_pp-1_tp-1_dp-1_cp-1] pp_groups: replica_groups.pp_groups=[[0], [1]]\n",
      "[2025-06-11 00:07:46.105: I neuronx_distributed/parallel_layers/parallel_state.py:631] [rank_0_pp-1_tp-1_dp-1_cp-1] cp_groups: replica_groups.cp_groups=[[0], [1]]\n",
      "[2025-06-11 00:07:46.105: I neuronx_distributed/parallel_layers/parallel_state.py:632] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_model_groups: replica_groups.ep_model_groups=[[0], [1]]\n",
      "[2025-06-11 00:07:46.106: I neuronx_distributed/parallel_layers/parallel_state.py:633] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_data_groups: replica_groups.ep_data_groups=[[0], [1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Neuron:Generating 1 hlos for key: context_encoding_model\n",
      "INFO:Neuron:Started loading module context_encoding_model\n",
      "INFO:Neuron:Finished loading module context_encoding_model in 0.11491250991821289 seconds\n",
      "INFO:Neuron:generating HLO: context_encoding_model, input example shape = torch.Size([1, 64])\n",
      "/opt/aws_neuronx_venv_pytorch_2_5/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:476: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "/opt/aws_neuronx_venv_pytorch_2_5/lib/python3.10/site-packages/optimum/neuron/models/inference/nxd/backend/modules/generation/sampling.py:302: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  probs_cumsum = cumsum(tensor_in=probs_soft_max, dim=dim, on_cpu=self.on_cpu)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add predicate {{0,+,-1}<i0=[0:128:1]>,+,0}<i1=[0:2048:1]>\n",
      "start lb and ub of  {0,+,-1}<i0=[0:128:1]> is 0 0\n",
      "Add predicate {{255,+,0}<i0=[0:128:1]>,+,-1}<i1=[0:2048:1]>\n",
      "start lb and ub of  {{255,+,0}<i0=[0:128:1]>,+,-1}<i1=[0:2048:1]> is 255 255\n",
      "before build_invert_ranges alive full {\n",
      "  0 <= i1=[0:2048:1] <= 2047; alive full {\n",
      "    0 <= i1=[0:2048:1] <= 2047; 1 <= i0=[0:128:1] <= 127; alive leaf\n",
      "  }\n",
      "  256 <= i1=[0:2048:1] <= 2047; alive {\n",
      "    256 <= i1=[0:2048:1] <= 2047; 0 <= i0=[0:128:1] <= 127; alive full leaf\n",
      "  }\n",
      "}\n",
      "generated domains alive full {\n",
      "  0 <= i1=[0:2048:1] <= 255; alive {\n",
      "    0 <= i1=[0:2048:1] <= 255; 0 <= i0=[0:128:1] <= 0; alive leaf\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/aws_neuronx_venv_pytorch_2_5/lib/python3.10/site-packages/optimum/neuron/models/inference/nxd/backend/modules/generation/sampling.py:265: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  probs_cumsum = cumsum(tensor_in=probs, dim=dim, on_cpu=self.on_cpu)\n",
      "/opt/aws_neuronx_venv_pytorch_2_5/lib/python3.10/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:158: UserWarning: Received an input tensor that was unused. Tensor will be ignored. (index=1, shape=torch.Size([1, 64]), dtype=torch.int32)\n",
      "  warnings.warn(\n",
      "/opt/aws_neuronx_venv_pytorch_2_5/lib/python3.10/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:158: UserWarning: Received an input tensor that was unused. Tensor will be ignored. (index=3, shape=torch.Size([1]), dtype=torch.int32)\n",
      "  warnings.warn(\n",
      "INFO:Neuron:Generating 1 hlos for key: token_generation_model\n",
      "INFO:Neuron:Started loading module token_generation_model\n",
      "INFO:Neuron:Finished loading module token_generation_model in 0.09456944465637207 seconds\n",
      "INFO:Neuron:generating HLO: token_generation_model, input example shape = torch.Size([1, 1])\n",
      "/opt/aws_neuronx_venv_pytorch_2_5/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:476: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "/opt/aws_neuronx_venv_pytorch_2_5/lib/python3.10/site-packages/optimum/neuron/models/inference/nxd/backend/modules/generation/sampling.py:302: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  probs_cumsum = cumsum(tensor_in=probs_soft_max, dim=dim, on_cpu=self.on_cpu)\n",
      "/opt/aws_neuronx_venv_pytorch_2_5/lib/python3.10/site-packages/optimum/neuron/models/inference/nxd/backend/modules/generation/sampling.py:265: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  probs_cumsum = cumsum(tensor_in=probs, dim=dim, on_cpu=self.on_cpu)\n",
      "/opt/aws_neuronx_venv_pytorch_2_5/lib/python3.10/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:158: UserWarning: Received an input tensor that was unused. Tensor will be ignored. (index=3, shape=torch.Size([1]), dtype=torch.int32)\n",
      "  warnings.warn(\n",
      "INFO:Neuron:Started compilation for all HLOs\n",
      "neuronxcc-2.17.194.0+d312836f/MODULE_d5a6dd81deb29bd84793+165e9558/model.done not found in aws-neuron/optimum-neuron-cache: the corresponding graph will be recompiled. This may take up to one hour for large models.\n",
      "neuronxcc-2.17.194.0+d312836f/MODULE_d5a6dd81deb29bd84793+165e9558/model.done not found in aws-neuron/optimum-neuron-cache: the corresponding graph will be recompiled. This may take up to one hour for large models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "......Completed run_backend_driver.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Neuron:Caching neff at /var/tmp/neuron-compile-cache/neuronxcc-2.17.194.0+d312836f/MODULE_d5a6dd81deb29bd84793+165e9558/model.neff\n",
      "INFO:Neuron:Caching wrapped neff HLO stub at /var/tmp/neuron-compile-cache/neuronxcc-2.17.194.0+d312836f/MODULE_d5a6dd81deb29bd84793+165e9558/model.neff\n",
      "INFO:Neuron:Done compilation for the priority HLO\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Compiler status PASS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Neuron:Updating the hlo module with optimized layout\n",
      "INFO:Neuron:Done optimizing weight layout for all HLOs\n",
      "neuronxcc-2.17.194.0+d312836f/MODULE_46dd3ff446ebe0729059+bfe5714b/model.done not found in aws-neuron/optimum-neuron-cache: the corresponding graph will be recompiled. This may take up to one hour for large models.\n",
      "neuronxcc-2.17.194.0+d312836f/MODULE_46dd3ff446ebe0729059+bfe5714b/model.done not found in aws-neuron/optimum-neuron-cache: the corresponding graph will be recompiled. This may take up to one hour for large models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "......."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Neuron:Caching neff at /var/tmp/neuron-compile-cache/neuronxcc-2.17.194.0+d312836f/MODULE_46dd3ff446ebe0729059+bfe5714b/model.neff\n",
      "INFO:Neuron:Finished Compilation for all HLOs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed run_backend_driver.\n",
      "\n",
      "Compiler status PASS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "neuronxcc-2.17.194.0+d312836f/MODULE_929a13d0e6572c3926fb+431f5505/wrapped_neff.hlo not found in aws-neuron/optimum-neuron-cache: the corresponding graph will be recompiled. This may take up to one hour for large models.\n",
      "INFO:Neuron:Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.17.194.0+d312836f/MODULE_929a13d0e6572c3926fb+431f5505/model.neff\n",
      "INFO:Neuron:Done preparing weight layout transformation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8135ca8e453c44c4a8ac90870340c849",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/3.44G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9a809a754254a9cb39e5a0d54257adf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/622M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef8401d4cc9e4966b2df5f034b4a5344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/25.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are no saved sharded checkpoints.\n",
      "Sharding and loading rank 0\n",
      "[2025-06-11 00:12:39.766: I neuronx_distributed/parallel_layers/parallel_state.py:588] > initializing tensor model parallel with size 2\n",
      "[2025-06-11 00:12:39.767: I neuronx_distributed/parallel_layers/parallel_state.py:589] > initializing pipeline model parallel with size 1\n",
      "[2025-06-11 00:12:39.768: I neuronx_distributed/parallel_layers/parallel_state.py:590] > initializing context model parallel with size 1\n",
      "[2025-06-11 00:12:39.768: I neuronx_distributed/parallel_layers/parallel_state.py:591] > initializing data parallel with size 1\n",
      "[2025-06-11 00:12:39.769: I neuronx_distributed/parallel_layers/parallel_state.py:592] > initializing world size to 2\n",
      "[2025-06-11 00:12:39.771: I neuronx_distributed/parallel_layers/parallel_state.py:339] [rank_0_pp-1_tp-1_dp-1_cp-1] Chosen Logic for replica groups ret_logic=<PG_Group_Logic.LOGIC1: (<function ascending_ring_PG_group at 0x7f1c53999090>, 'Ascending Ring PG Group')>\n",
      "[2025-06-11 00:12:39.774: I neuronx_distributed/parallel_layers/parallel_state.py:628] [rank_0_pp-1_tp-1_dp-1_cp-1] tp_groups: replica_groups.tp_groups=[[0, 1]]\n",
      "[2025-06-11 00:12:39.774: I neuronx_distributed/parallel_layers/parallel_state.py:629] [rank_0_pp-1_tp-1_dp-1_cp-1] dp_groups: replica_groups.dp_groups=[[0], [1]]\n",
      "[2025-06-11 00:12:39.775: I neuronx_distributed/parallel_layers/parallel_state.py:630] [rank_0_pp-1_tp-1_dp-1_cp-1] pp_groups: replica_groups.pp_groups=[[0], [1]]\n",
      "[2025-06-11 00:12:39.775: I neuronx_distributed/parallel_layers/parallel_state.py:631] [rank_0_pp-1_tp-1_dp-1_cp-1] cp_groups: replica_groups.cp_groups=[[0], [1]]\n",
      "[2025-06-11 00:12:39.776: I neuronx_distributed/parallel_layers/parallel_state.py:632] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_model_groups: replica_groups.ep_model_groups=[[0], [1]]\n",
      "[2025-06-11 00:12:39.776: I neuronx_distributed/parallel_layers/parallel_state.py:633] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_data_groups: replica_groups.ep_data_groups=[[0], [1]]\n",
      "Sharding and loading rank 1\n",
      "[2025-06-11 00:12:47.371: I neuronx_distributed/parallel_layers/parallel_state.py:588] > initializing tensor model parallel with size 2\n",
      "[2025-06-11 00:12:47.371: I neuronx_distributed/parallel_layers/parallel_state.py:589] > initializing pipeline model parallel with size 1\n",
      "[2025-06-11 00:12:47.372: I neuronx_distributed/parallel_layers/parallel_state.py:590] > initializing context model parallel with size 1\n",
      "[2025-06-11 00:12:47.373: I neuronx_distributed/parallel_layers/parallel_state.py:591] > initializing data parallel with size 1\n",
      "[2025-06-11 00:12:47.373: I neuronx_distributed/parallel_layers/parallel_state.py:592] > initializing world size to 2\n",
      "[2025-06-11 00:12:47.375: I neuronx_distributed/parallel_layers/parallel_state.py:339] [rank_0_pp-1_tp-1_dp-1_cp-1] Chosen Logic for replica groups ret_logic=<PG_Group_Logic.LOGIC1: (<function ascending_ring_PG_group at 0x7f1c53999090>, 'Ascending Ring PG Group')>\n",
      "[2025-06-11 00:12:47.375: I neuronx_distributed/parallel_layers/parallel_state.py:628] [rank_0_pp-1_tp-1_dp-1_cp-1] tp_groups: replica_groups.tp_groups=[[0, 1]]\n",
      "[2025-06-11 00:12:47.376: I neuronx_distributed/parallel_layers/parallel_state.py:629] [rank_0_pp-1_tp-1_dp-1_cp-1] dp_groups: replica_groups.dp_groups=[[0], [1]]\n",
      "[2025-06-11 00:12:47.377: I neuronx_distributed/parallel_layers/parallel_state.py:630] [rank_0_pp-1_tp-1_dp-1_cp-1] pp_groups: replica_groups.pp_groups=[[0], [1]]\n",
      "[2025-06-11 00:12:47.377: I neuronx_distributed/parallel_layers/parallel_state.py:631] [rank_0_pp-1_tp-1_dp-1_cp-1] cp_groups: replica_groups.cp_groups=[[0], [1]]\n",
      "[2025-06-11 00:12:47.378: I neuronx_distributed/parallel_layers/parallel_state.py:632] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_model_groups: replica_groups.ep_model_groups=[[0], [1]]\n",
      "[2025-06-11 00:12:47.378: I neuronx_distributed/parallel_layers/parallel_state.py:633] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_data_groups: replica_groups.ep_data_groups=[[0], [1]]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "from optimum.neuron import NeuronModelForCausalLM\n",
    "\n",
    "# Just use the config as-is\n",
    "config = AutoConfig.from_pretrained(\"Qwen/Qwen3-1.7B\")\n",
    "\n",
    "compiler_args = {\"num_cores\": 2, \"auto_cast_type\": 'bf16'}\n",
    "input_shapes = {\"batch_size\": 1, \"sequence_length\": 64}\n",
    "\n",
    "model = NeuronModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen3-1.7B\",\n",
    "    export=True,\n",
    "    config=config,  # Use original config with head_dim=128\n",
    "    **compiler_args,\n",
    "    **input_shapes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63ddcd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"Qwen3-1.7B-neuron\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "762a9e7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7ffe32a13454830bfd340611bdddf70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login(new_session=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f79155c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea1613a4444f4c5a89742c9a4ffba84d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b192144ecc41440d8caa9b4fc7bf5eb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.pt:   0%|          | 0.00/4.08G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import whoami\n",
    "\n",
    "org = whoami()['name']\n",
    "repo_id = f\"{org}/Qwen3-1.7B-neuron\"\n",
    "model.push_to_hub(\"Qwen3-1.7B-neuron\", repository_id=repo_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1a7c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:MASTER_ADDR environment variable is not set, defaulting to localhost\n",
      "WARNING:root:Found libneuronpjrt.so. Setting PJRT_DEVICE=NEURON.\n",
      "/opt/aws_neuronx_venv_pytorch_2_5/lib/python3.10/site-packages/neuronx_distributed/modules/moe/expert_mlps.py:11: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from neuronx_distributed.modules.moe.blockwise import (\n",
      "/opt/aws_neuronx_venv_pytorch_2_5/lib/python3.10/site-packages/neuronx_distributed/modules/moe/expert_mlps.py:11: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from neuronx_distributed.modules.moe.blockwise import (\n",
      "/opt/aws_neuronx_venv_pytorch_2_5/lib/python3.10/site-packages/neuronx_distributed/modules/moe/expert_mlps.py:11: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from neuronx_distributed.modules.moe.blockwise import (\n",
      "/opt/aws_neuronx_venv_pytorch_2_5/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: Set seed for `privateuseone` device does not take effect, please add API's `_is_in_bad_fork` and `manual_seed_all` to `privateuseone` device module.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/aws_neuronx_venv_pytorch_2_5/lib/python3.10/site-packages/optimum/neuron/models/inference/nxd/backend/modules/attention/attention_base.py:51: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from .gqa import GQA, GroupQueryAttention_O, GroupQueryAttention_QKV  # noqa: E402\n",
      "/opt/aws_neuronx_venv_pytorch_2_5/lib/python3.10/site-packages/optimum/neuron/models/inference/nxd/llama/modeling_llama.py:47: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from ..backend.modules.attention.attention_base import NeuronAttentionBase\n",
      "INFO:Neuron:Checkpoint file not found in Qwen3-1.7B-neuron, trying to load from HuggingFace Hub.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are no saved sharded checkpoints.\n",
      "Sharding and loading rank 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "199b5ab28feb45e69988a7a14eca4705",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/3.44G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "542a8eb064e24335a62faf49e9ab47df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/622M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ced3ebb250741f4a5c87bb40f870926",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/25.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are no saved sharded checkpoints.\n",
      "Sharding and loading rank 0\n",
      "[2025-06-10 23:57:04.479: I neuronx_distributed/parallel_layers/parallel_state.py:588] > initializing tensor model parallel with size 2\n",
      "[2025-06-10 23:57:04.480: I neuronx_distributed/parallel_layers/parallel_state.py:589] > initializing pipeline model parallel with size 1\n",
      "[2025-06-10 23:57:04.481: I neuronx_distributed/parallel_layers/parallel_state.py:590] > initializing context model parallel with size 1\n",
      "[2025-06-10 23:57:04.481: I neuronx_distributed/parallel_layers/parallel_state.py:591] > initializing data parallel with size 1\n",
      "[2025-06-10 23:57:04.482: I neuronx_distributed/parallel_layers/parallel_state.py:592] > initializing world size to 2\n",
      "[2025-06-10 23:57:04.485: I neuronx_distributed/parallel_layers/parallel_state.py:339] [rank_0_pp-1_tp-1_dp-1_cp-1] Chosen Logic for replica groups ret_logic=<PG_Group_Logic.LOGIC1: (<function ascending_ring_PG_group at 0x7f66e835d510>, 'Ascending Ring PG Group')>\n",
      "[2025-06-10 23:57:04.485: I neuronx_distributed/parallel_layers/parallel_state.py:628] [rank_0_pp-1_tp-1_dp-1_cp-1] tp_groups: replica_groups.tp_groups=[[0, 1]]\n",
      "[2025-06-10 23:57:04.486: I neuronx_distributed/parallel_layers/parallel_state.py:629] [rank_0_pp-1_tp-1_dp-1_cp-1] dp_groups: replica_groups.dp_groups=[[0], [1]]\n",
      "[2025-06-10 23:57:04.487: I neuronx_distributed/parallel_layers/parallel_state.py:630] [rank_0_pp-1_tp-1_dp-1_cp-1] pp_groups: replica_groups.pp_groups=[[0], [1]]\n",
      "[2025-06-10 23:57:04.487: I neuronx_distributed/parallel_layers/parallel_state.py:631] [rank_0_pp-1_tp-1_dp-1_cp-1] cp_groups: replica_groups.cp_groups=[[0], [1]]\n",
      "[2025-06-10 23:57:04.488: I neuronx_distributed/parallel_layers/parallel_state.py:632] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_model_groups: replica_groups.ep_model_groups=[[0], [1]]\n",
      "[2025-06-10 23:57:04.488: I neuronx_distributed/parallel_layers/parallel_state.py:633] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_data_groups: replica_groups.ep_data_groups=[[0], [1]]\n",
      "After parent init:\n",
      "  num_heads: 8\n",
      "  num_key_value_heads: 4\n",
      "  num_key_value_groups: 2\n",
      "  head_dim: 128\n",
      "After parent init:\n",
      "  num_heads: 8\n",
      "  num_key_value_heads: 4\n",
      "  num_key_value_groups: 2\n",
      "  head_dim: 128\n",
      "After parent init:\n",
      "  num_heads: 8\n",
      "  num_key_value_heads: 4\n",
      "  num_key_value_groups: 2\n",
      "  head_dim: 128\n",
      "After parent init:\n",
      "  num_heads: 8\n",
      "  num_key_value_heads: 4\n",
      "  num_key_value_groups: 2\n",
      "  head_dim: 128\n",
      "After parent init:\n",
      "  num_heads: 8\n",
      "  num_key_value_heads: 4\n",
      "  num_key_value_groups: 2\n",
      "  head_dim: 128\n",
      "After parent init:\n",
      "  num_heads: 8\n",
      "  num_key_value_heads: 4\n",
      "  num_key_value_groups: 2\n",
      "  head_dim: 128\n",
      "After parent init:\n",
      "  num_heads: 8\n",
      "  num_key_value_heads: 4\n",
      "  num_key_value_groups: 2\n",
      "  head_dim: 128\n",
      "After parent init:\n",
      "  num_heads: 8\n",
      "  num_key_value_heads: 4\n",
      "  num_key_value_groups: 2\n",
      "  head_dim: 128\n",
      "After parent init:\n",
      "  num_heads: 8\n",
      "  num_key_value_heads: 4\n",
      "  num_key_value_groups: 2\n",
      "  head_dim: 128\n",
      "After parent init:\n",
      "  num_heads: 8\n",
      "  num_key_value_heads: 4\n",
      "  num_key_value_groups: 2\n",
      "  head_dim: 128\n",
      "After parent init:\n",
      "  num_heads: 8\n",
      "  num_key_value_heads: 4\n",
      "  num_key_value_groups: 2\n",
      "  head_dim: 128\n",
      "After parent init:\n",
      "  num_heads: 8\n",
      "  num_key_value_heads: 4\n",
      "  num_key_value_groups: 2\n",
      "  head_dim: 128\n",
      "After parent init:\n",
      "  num_heads: 8\n",
      "  num_key_value_heads: 4\n",
      "  num_key_value_groups: 2\n",
      "  head_dim: 128\n",
      "After parent init:\n",
      "  num_heads: 8\n",
      "  num_key_value_heads: 4\n",
      "  num_key_value_groups: 2\n",
      "  head_dim: 128\n",
      "After parent init:\n",
      "  num_heads: 8\n",
      "  num_key_value_heads: 4\n",
      "  num_key_value_groups: 2\n",
      "  head_dim: 128\n",
      "After parent init:\n",
      "  num_heads: 8\n",
      "  num_key_value_heads: 4\n",
      "  num_key_value_groups: 2\n",
      "  head_dim: 128\n",
      "After parent init:\n",
      "  num_heads: 8\n",
      "  num_key_value_heads: 4\n",
      "  num_key_value_groups: 2\n",
      "  head_dim: 128\n",
      "After parent init:\n",
      "  num_heads: 8\n",
      "  num_key_value_heads: 4\n",
      "  num_key_value_groups: 2\n",
      "  head_dim: 128\n",
      "After parent init:\n",
      "  num_heads: 8\n",
      "  num_key_value_heads: 4\n",
      "  num_key_value_groups: 2\n",
      "  head_dim: 128\n",
      "After parent init:\n",
      "  num_heads: 8\n",
      "  num_key_value_heads: 4\n",
      "  num_key_value_groups: 2\n",
      "  head_dim: 128\n",
      "After parent init:\n",
      "  num_heads: 8\n",
      "  num_key_value_heads: 4\n",
      "  num_key_value_groups: 2\n",
      "  head_dim: 128\n",
      "After parent init:\n",
      "  num_heads: 8\n",
      "  num_key_value_heads: 4\n",
      "  num_key_value_groups: 2\n",
      "  head_dim: 128\n",
      "After parent init:\n",
      "  num_heads: 8\n",
      "  num_key_value_heads: 4\n",
      "  num_key_value_groups: 2\n",
      "  head_dim: 128\n",
      "After parent init:\n",
      "  num_heads: 8\n",
      "  num_key_value_heads: 4\n",
      "  num_key_value_groups: 2\n",
      "  head_dim: 128\n",
      "After parent init:\n",
      "  num_heads: 8\n",
      "  num_key_value_heads: 4\n",
      "  num_key_value_groups: 2\n",
      "  head_dim: 128\n",
      "After parent init:\n",
      "  num_heads: 8\n",
      "  num_key_value_heads: 4\n",
      "  num_key_value_groups: 2\n",
      "  head_dim: 128\n",
      "After parent init:\n",
      "  num_heads: 8\n",
      "  num_key_value_heads: 4\n",
      "  num_key_value_groups: 2\n",
      "  head_dim: 128\n",
      "After parent init:\n",
      "  num_heads: 8\n",
      "  num_key_value_heads: 4\n",
      "  num_key_value_groups: 2\n",
      "  head_dim: 128\n",
      "Sharding and loading rank 1\n",
      "[2025-06-10 23:57:05.755: I neuronx_distributed/parallel_layers/parallel_state.py:588] > initializing tensor model parallel with size 2\n",
      "[2025-06-10 23:57:05.756: I neuronx_distributed/parallel_layers/parallel_state.py:589] > initializing pipeline model parallel with size 1\n",
      "[2025-06-10 23:57:05.756: I neuronx_distributed/parallel_layers/parallel_state.py:590] > initializing context model parallel with size 1\n",
      "[2025-06-10 23:57:05.757: I neuronx_distributed/parallel_layers/parallel_state.py:591] > initializing data parallel with size 1\n",
      "[2025-06-10 23:57:05.757: I neuronx_distributed/parallel_layers/parallel_state.py:592] > initializing world size to 2\n",
      "[2025-06-10 23:57:05.759: I neuronx_distributed/parallel_layers/parallel_state.py:339] [rank_0_pp-1_tp-1_dp-1_cp-1] Chosen Logic for replica groups ret_logic=<PG_Group_Logic.LOGIC1: (<function ascending_ring_PG_group at 0x7f66e835d510>, 'Ascending Ring PG Group')>\n",
      "[2025-06-10 23:57:05.760: I neuronx_distributed/parallel_layers/parallel_state.py:628] [rank_0_pp-1_tp-1_dp-1_cp-1] tp_groups: replica_groups.tp_groups=[[0, 1]]\n",
      "[2025-06-10 23:57:05.761: I neuronx_distributed/parallel_layers/parallel_state.py:629] [rank_0_pp-1_tp-1_dp-1_cp-1] dp_groups: replica_groups.dp_groups=[[0], [1]]\n",
      "[2025-06-10 23:57:05.761: I neuronx_distributed/parallel_layers/parallel_state.py:630] [rank_0_pp-1_tp-1_dp-1_cp-1] pp_groups: replica_groups.pp_groups=[[0], [1]]\n",
      "[2025-06-10 23:57:05.762: I neuronx_distributed/parallel_layers/parallel_state.py:631] [rank_0_pp-1_tp-1_dp-1_cp-1] cp_groups: replica_groups.cp_groups=[[0], [1]]\n",
      "[2025-06-10 23:57:05.763: I neuronx_distributed/parallel_layers/parallel_state.py:632] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_model_groups: replica_groups.ep_model_groups=[[0], [1]]\n",
      "[2025-06-10 23:57:05.764: I neuronx_distributed/parallel_layers/parallel_state.py:633] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_data_groups: replica_groups.ep_data_groups=[[0], [1]]\n",
      "After parent init:\n",
      "  num_heads: 8\n",
      "  num_key_value_heads: 4\n",
      "  num_key_value_groups: 2\n",
      "  head_dim: 128\n",
      "After parent init:\n",
      "  num_heads: 8\n",
      "  num_key_value_heads: 4\n",
      "  num_key_value_groups: 2\n",
      "  head_dim: 128\n",
      "After parent init:\n",
      "  num_heads: 8\n",
      "  num_key_value_heads: 4\n",
      "  num_key_value_groups: 2\n",
      "  head_dim: 128\n",
      "After parent init:\n",
      "  num_heads: 8\n",
      "  num_key_value_heads: 4\n",
      "  num_key_value_groups: 2\n",
      "  head_dim: 128\n",
      "After parent init:\n",
      "  num_heads: 8\n",
      "  num_key_value_heads: 4\n",
      "  num_key_value_groups: 2\n",
      "  head_dim: 128\n",
      "After parent init:\n",
      "  num_heads: 8\n",
      "  num_key_value_heads: 4\n",
      "  num_key_value_groups: 2\n",
      "  head_dim: 128\n",
      "After parent init:\n",
      "  num_heads: 8\n",
      "  num_key_value_heads: 4\n",
      "  num_key_value_groups: 2\n",
      "  head_dim: 128\n",
      "After parent init:\n",
      "  num_heads: 8\n",
      "  num_key_value_heads: 4\n",
      "  num_key_value_groups: 2\n",
      "  head_dim: 128\n",
      "After parent init:\n",
      "  num_heads: 8\n",
      "  num_key_value_heads: 4\n",
      "  num_key_value_groups: 2\n",
      "  head_dim: 128\n",
      "After parent init:\n",
      "  num_heads: 8\n",
      "  num_key_value_heads: 4\n",
      "  num_key_value_groups: 2\n",
      "  head_dim: 128\n",
      "After parent init:\n",
      "  num_heads: 8\n",
      "  num_key_value_heads: 4\n",
      "  num_key_value_groups: 2\n",
      "  head_dim: 128\n",
      "After parent init:\n",
      "  num_heads: 8\n",
      "  num_key_value_heads: 4\n",
      "  num_key_value_groups: 2\n",
      "  head_dim: 128\n",
      "After parent init:\n",
      "  num_heads: 8\n",
      "  num_key_value_heads: 4\n",
      "  num_key_value_groups: 2\n",
      "  head_dim: 128\n",
      "After parent init:\n",
      "  num_heads: 8\n",
      "  num_key_value_heads: 4\n",
      "  num_key_value_groups: 2\n",
      "  head_dim: 128\n",
      "After parent init:\n",
      "  num_heads: 8\n",
      "  num_key_value_heads: 4\n",
      "  num_key_value_groups: 2\n",
      "  head_dim: 128\n",
      "After parent init:\n",
      "  num_heads: 8\n",
      "  num_key_value_heads: 4\n",
      "  num_key_value_groups: 2\n",
      "  head_dim: 128\n",
      "After parent init:\n",
      "  num_heads: 8\n",
      "  num_key_value_heads: 4\n",
      "  num_key_value_groups: 2\n",
      "  head_dim: 128\n",
      "After parent init:\n",
      "  num_heads: 8\n",
      "  num_key_value_heads: 4\n",
      "  num_key_value_groups: 2\n",
      "  head_dim: 128\n",
      "After parent init:\n",
      "  num_heads: 8\n",
      "  num_key_value_heads: 4\n",
      "  num_key_value_groups: 2\n",
      "  head_dim: 128\n",
      "After parent init:\n",
      "  num_heads: 8\n",
      "  num_key_value_heads: 4\n",
      "  num_key_value_groups: 2\n",
      "  head_dim: 128\n",
      "After parent init:\n",
      "  num_heads: 8\n",
      "  num_key_value_heads: 4\n",
      "  num_key_value_groups: 2\n",
      "  head_dim: 128\n",
      "After parent init:\n",
      "  num_heads: 8\n",
      "  num_key_value_heads: 4\n",
      "  num_key_value_groups: 2\n",
      "  head_dim: 128\n",
      "After parent init:\n",
      "  num_heads: 8\n",
      "  num_key_value_heads: 4\n",
      "  num_key_value_groups: 2\n",
      "  head_dim: 128\n",
      "After parent init:\n",
      "  num_heads: 8\n",
      "  num_key_value_heads: 4\n",
      "  num_key_value_groups: 2\n",
      "  head_dim: 128\n",
      "After parent init:\n",
      "  num_heads: 8\n",
      "  num_key_value_heads: 4\n",
      "  num_key_value_groups: 2\n",
      "  head_dim: 128\n",
      "After parent init:\n",
      "  num_heads: 8\n",
      "  num_key_value_heads: 4\n",
      "  num_key_value_groups: 2\n",
      "  head_dim: 128\n",
      "After parent init:\n",
      "  num_heads: 8\n",
      "  num_key_value_heads: 4\n",
      "  num_key_value_groups: 2\n",
      "  head_dim: 128\n",
      "After parent init:\n",
      "  num_heads: 8\n",
      "  num_key_value_heads: 4\n",
      "  num_key_value_groups: 2\n",
      "  head_dim: 128\n"
     ]
    }
   ],
   "source": [
    "from optimum.neuron import NeuronModelForCausalLM\n",
    "\n",
    "try:\n",
    "    model\n",
    "except NameError:\n",
    "    # Edit this to use another base model\n",
    "    model = NeuronModelForCausalLM.from_pretrained(\"Qwen3-1.7B-neuron\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "832d93bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-1.7B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5dce27aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What is deep-learning ? What is the difference between deep learning and machine learning? What is the difference between deep learning and neural networks? What is the difference between deep learning and artificial intelligence']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\"What is deep-learning ?\", return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs,\n",
    "                         max_new_tokens=32,\n",
    "                         do_sample=True,\n",
    "                         temperature=0.1,\n",
    "                         top_k=50,\n",
    "                         top_p=0.9)\n",
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea076ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is Deep Learning? A Deep Learning is a subset of machine learning that uses neural networks with multiple layers to'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"What is Deep Learning?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, do_sample=False, max_new_tokens=17)\n",
    "generated_text = tokenizer.decode(outputs[0])\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166e0b40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws_neuronx_venv_pytorch_2_5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
